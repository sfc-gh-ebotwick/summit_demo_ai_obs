{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "lastEditStatus": {
   "notebookId": "767vd4eemrt73r5iccqs",
   "authorId": "5095547476787",
   "authorName": "EBOTWICK",
   "authorEmail": "elliott.botwick@snowflake.com",
   "sessionId": "263f6947-840d-4131-a250-cd8abc8208f6",
   "lastEditTime": 1747955403320
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Libraries",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snowpark\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "id": "817c3622-da65-4ee3-9f1f-da15e94ec8d7",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "DB_NAME = \"SUMMIT_AI_OBS_DEMO\"\nSCHEMA_NAME = \"DATA\"\nSTAGE_NAME = \"DOCS\"\nWH_NAME = \"AI_OBS_WAREHOUSE\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64842e12-1e4c-423b-a568-376102123485",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "list_files_in_stage",
    "resultHeight": 426
   },
   "outputs": [],
   "source": "-- List files in the stage to identify PDFs\nLS @{{DB_NAME}}.{{SCHEMA_NAME}}.{{STAGE_NAME}}"
  },
  {
   "cell_type": "code",
   "id": "4533b750-4e92-441b-8c21-4ed0c71b7384",
   "metadata": {
    "language": "python",
    "name": "upload_pdfs_to_stage",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import os\n\n#Define quick function to add local file to snowflake stage\ndef upload_file_to_stage(filename: str, target_stage: str):\n    print(f\"Adding file {filename} to {target_stage} stage...\")\n    put_result =  session.file.put(\n        local_file_name = f\"pdfs/{filename}\",\n        stage_location =  f'@\"{target_stage}\"',\n        auto_compress = False,\n        source_compression= 'AUTO_DETECT',\n        overwrite = True)\n    return put_result\n\n\nlocal_pdfs = []\nfor root, dirs, files  in os.walk('pdfs/'):\n    for file in files:\n        if file.endswith('.pdf'):\n            local_pdfs.append(file)\n\n\n#Get list of filenames already in stage\nstage_pdfs = [row[0][row[0].find('/')+1:] for row in session.sql(f'LS @{DB_NAME}.{SCHEMA_NAME}.{STAGE_NAME}').select('\"name\"').collect()]\n\nfor pdf in local_pdfs:\n    if pdf in stage_pdfs:\n        print(f\"File {pdf} already in {STAGE_NAME} stage!\")\n    else:\n        upload_file_to_stage(pdf, STAGE_NAME)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d25663e4-08a0-418c-a6d4-96e913aef549",
   "metadata": {
    "collapsed": false,
    "name": "Step1"
   },
   "source": [
    "## Step 1: Parse and Chunk Text from PDFs\n",
    "We begin by parsing the content of uploaded PDFs and chunking the text using Snowflake's [PARSED_TEXT](https://docs.snowflake.com/sql-reference/functions/parse_document-snowflake-cortex) and [SPLIT_TEXT_RECURSIVE_CHARACTER](https://docs.snowflake.com/sql-reference/functions/split_text_recursive_character-snowflake-cortex) features. These steps structure the text into manageable segments optimized for retrieval. To ensure that the PDF parsing and chunking have been processed correctly, we run queries on the parsed and chunked tables. This step helps verify the integrity of the content.\n",
    "\n",
    "Objective: **Transform unstructured content into indexed chunks for efficient search and retrieval.**\n",
    "\n",
    "Key Outputs:\n",
    "- SKO.HOP.PARSED_TEXT: Table containing the raw text.\n",
    "- SKO.HOP.CORTEX_CHUNK: Chunked, searchable content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f293bf7-06b5-4d05-9666-ad3096d25a31",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "CreateParsedTextTable",
    "resultHeight": 111
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (579301450.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    -- Create a table to hold the extracted text from the PDF files loaded in the SKO.HOP.RAG stage\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": "-- Create a table to hold the extracted text from the PDF files\nCREATE OR REPLACE TABLE {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT (relative_path VARCHAR(500), raw_text VARIANT);"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e7c0b-6c8f-472f-9cfe-94e1351b9925",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "UseParseDocument",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "INSERT INTO {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT (relative_path, raw_text)\nWITH pdf_files AS (\n    SELECT DISTINCT\n        METADATA$FILENAME AS relative_path\n    FROM @{{DB_NAME}}.{{SCHEMA_NAME}}.DOCS\n    WHERE METADATA$FILENAME ILIKE '%.pdf'\n      -- Exclude files that have already been parsed\n      AND METADATA$FILENAME NOT IN (SELECT relative_path FROM PARSED_TEXT)\n)\nSELECT \n    relative_path,\n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n        '@{{DB_NAME}}.{{SCHEMA_NAME}}.DOCS',  -- Your stage name\n        relative_path,  -- File path\n        {'mode': 'layout'}  -- Adjust mode as needed ('layout', 'ocr')\n    ) AS raw_text\nFROM pdf_files;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fe17b-9580-43b4-8b23-fb61a695df6c",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "ViewParsedText"
   },
   "outputs": [],
   "source": "-- inspect the results and count the tokens for each document\nSELECT *, SNOWFLAKE.CORTEX.COUNT_TOKENS('mistral-7b', RAW_TEXT) as token_count\nFROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce5839-f0eb-4d4f-838e-9b3633979c8c",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "TestingSplitTextFunction",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Chunk the text based on paragraph seperators and write into DOC_CHUNKS_TABLE;\n\nCREATE OR REPLACE TABLE {{DB_NAME}}.{{SCHEMA_NAME}}.DOC_CHUNKS AS\nWITH text_chunks AS (\n    SELECT\n        relative_path,\n        SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n            raw_text:content::STRING,  -- Extract the 'content' field from the JSON\n            'markdown', -- Adjust to 'markdown' if needed\n            2000,       -- Adjust chunk size\n            100,        -- Adjust overlap size\n            ['\\n\\n', '\\n']    -- Adjust separators\n        ) AS chunks\n    FROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT\n)\nSELECT\n    relative_path,\n    c.value AS chunk  -- Extract each chunk of the parsed text\nFROM text_chunks,\nLATERAL FLATTEN(INPUT => chunks) c;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cfb61-0168-43ea-9a70-8242dc45ec07",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "ViewChunkedText",
    "resultHeight": 438
   },
   "outputs": [],
   "source": "-- Check the results and \n\nSELECT *, SNOWFLAKE.CORTEX.COUNT_TOKENS('mistral-7b', CHUNK) as token_count\nFROM {{DB_NAME}}.{{SCHEMA_NAME}}.DOC_CHUNKS ;"
  },
  {
   "cell_type": "code",
   "id": "51ffd4c9-fd4d-441b-bd9b-d755025a1acf",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- Use cortex to classify the text chunk into categories\n\nALTER TABLE DOC_CHUNKS ADD COLUMN IF NOT EXISTS CHUNK_TOPIC VARCHAR(100);\n\nUPDATE DOC_CHUNKS \n    SET CHUNK_TOPIC = SNOWFLAKE.CORTEX.COMPLETE('llama4-maverick', \n    concat('Categorize the following text as one of the following categories \n    [Customer Reference, Code Example, Benchmark, Technical Blog] \n    and only return the name of the category. No additional text.', CHUNK));\n\nSELECT * FROM DOC_CHUNKS;    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e9ba5f5-d03b-44e8-a8d0-ed46770961f7",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- Concatenate the doc title, doc type, and text into a single column. This is what we will build our vector index on top of\nSELECT \n        ('DOCUMENT_TITLE: ' || RELATIVE_PATH || \n        '\\nDOCUMENT_TYPE: ' || CHUNK_TOPIC || \n        '\\nDOCUMENT_TEXT:\\n' || CHUNK) AS SEARCH_COL\nFROM {{DB_NAME}}.{{SCHEMA_NAME}}.DOC_CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a330f9-c774-47f2-b8ca-031ae441c602",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CreateCortexSearchService",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Create a search service over your new chunked pdf table\n\nCREATE OR REPLACE CORTEX SEARCH SERVICE {{DB_NAME}}.{{SCHEMA_NAME}}.SNOWFLAKE_BLOG_RETRIEVAL\n    ON SEARCH_COL\n    ATTRIBUTES CHUNK_TOPIC\n    WAREHOUSE = AI_OBS_WAREHOUSE\n    TARGET_LAG = '1 hour'\n    AS SELECT \n        RELATIVE_PATH,\n        CHUNK_TOPIC,\n        ('DOCUMENT_TITLE: ' || RELATIVE_PATH || \n        '\\nDOCUMENT_TYPE: ' || CHUNK_TOPIC || \n        '\\nDOCUMENT_TEXT:\\n' || CHUNK) AS SEARCH_COL\n    FROM {{DB_NAME}}.{{SCHEMA_NAME}}.DOC_CHUNKS;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19284d0-107c-4c7f-aa2e-5378159c4a3b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "CheckSearchResults"
   },
   "outputs": [],
   "source": "# Query your Snowflake Cortex Search Service using the Snowpark Python API to retrieve and process search results.\n\nfrom snowflake.snowpark import Session\nfrom snowflake.core import Root\nroot = Root(session)\n\ntranscript_search_service = (root\n  .databases[DB_NAME]\n  .schemas[SCHEMA_NAME]\n  .cortex_search_services['SNOWFLAKE_BLOG_RETRIEVAL']\n)\n\nresp = transcript_search_service.search(\n  query=\"\"\"Who has used cortex search?\"\"\",\n  columns=['SEARCH_COL'],\n  limit=3\n)\nresults = resp.results\n\ncontext_str = \"\"\nfor i, r in enumerate(results):\n    context_str += f\"Context document {i+1}: {r['SEARCH_COL']}\\n****************\\n\"\n\n    print(context_str)"
  },
  {
   "cell_type": "code",
   "id": "12ab7a53-14d3-433f-949a-1a5e0c2dddae",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# Query your Snowflake Cortex Search Service using the Snowpark Python API to retrieve and process search results.\nresp = transcript_search_service.search(\n  query=\"\"\"Who has used cortex search?\"\"\",\n  columns=['SEARCH_COL'],\n  limit=10,\n  experimental={\"returnConfidenceScores\": True})\n\n\nconfidence_score_threshold=2\n    \nfiltered_results = list(filter(lambda x: int(x['@CONFIDENCE_SCORE']) >=confidence_score_threshold, resp.results))\ncontext_chunks = list(map(lambda x: x['SEARCH_COL'], filtered_results))\n\nprint(f'Found {len(context_chunks)} results!')\n\nfor i in context_chunks:\n    print(i)\n    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n')",
   "execution_count": null
  }
 ]
}