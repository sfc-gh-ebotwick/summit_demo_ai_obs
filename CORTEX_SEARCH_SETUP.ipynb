{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "lastEditStatus": {
   "notebookId": "f32zmrkh7z3rylcgrott",
   "authorId": "5095547476787",
   "authorName": "EBOTWICK",
   "authorEmail": "elliott.botwick@snowflake.com",
   "sessionId": "fed2ce21-e6d3-4078-9e51-7914cb0a729c",
   "lastEditTime": 1746571380944
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Libraries",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snowpark\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "id": "817c3622-da65-4ee3-9f1f-da15e94ec8d7",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "DB_NAME = \"SUMMIT_25_AI_OBS_DEMO\"\nSCHEMA_NAME = \"DATA\"\nSTAGE_NAME = \"DOCS\"\nWH_NAME = \"COMPUTE_WH\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64842e12-1e4c-423b-a568-376102123485",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "ViewStage",
    "resultHeight": 426
   },
   "outputs": [],
   "source": "-- List files in the stage to identify PDFs\nLS @{{DB_NAME}}.{{SCHEMA_NAME}}.{{STAGE_NAME}}"
  },
  {
   "cell_type": "markdown",
   "id": "d25663e4-08a0-418c-a6d4-96e913aef549",
   "metadata": {
    "collapsed": false,
    "name": "Step1"
   },
   "source": [
    "## Step 1: Parse and Chunk Text from PDFs\n",
    "We begin by parsing the content of uploaded PDFs and chunking the text using Snowflake's [PARSED_TEXT](https://docs.snowflake.com/sql-reference/functions/parse_document-snowflake-cortex) and [SPLIT_TEXT_RECURSIVE_CHARACTER](https://docs.snowflake.com/sql-reference/functions/split_text_recursive_character-snowflake-cortex) features. These steps structure the text into manageable segments optimized for retrieval. To ensure that the PDF parsing and chunking have been processed correctly, we run queries on the parsed and chunked tables. This step helps verify the integrity of the content.\n",
    "\n",
    "Objective: **Transform unstructured content into indexed chunks for efficient search and retrieval.**\n",
    "\n",
    "Key Outputs:\n",
    "- SKO.HOP.PARSED_TEXT: Table containing the raw text.\n",
    "- SKO.HOP.CORTEX_CHUNK: Chunked, searchable content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f293bf7-06b5-4d05-9666-ad3096d25a31",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "CreateParsedTextTable",
    "resultHeight": 111
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (579301450.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    -- Create a table to hold the extracted text from the PDF files loaded in the SKO.HOP.RAG stage\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": "-- Create a table to hold the extracted text from the PDF files loaded in the SKO_SKORAGHOP_LIVE_PROD.HOP.RAG stage\n\n-- Complete the missing code (???) to use create a table called PARSED_TEXT\n\nCREATE OR REPLACE TABLE {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT (relative_path VARCHAR(500), raw_text VARIANT);"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e7c0b-6c8f-472f-9cfe-94e1351b9925",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "UseParseDocument",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "INSERT INTO SUMMIT_25_AI_OBS_DEMO.DATA.PARSED_TEXT (relative_path, raw_text)\nWITH pdf_files AS (\n    SELECT DISTINCT\n        METADATA$FILENAME AS relative_path\n    FROM @SUMMIT_25_AI_OBS_DEMO.DATA.DOCS\n    WHERE METADATA$FILENAME ILIKE '%.pdf'\n      -- Exclude files that have already been parsed\n      AND METADATA$FILENAME NOT IN (SELECT relative_path FROM PARSED_TEXT)\n)\nSELECT \n    relative_path,\n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n        '@SUMMIT_25_AI_OBS_DEMO.DATA.DOCS',  -- Your stage name\n        relative_path,  -- File path\n        {'mode': 'layout'}  -- Adjust mode as needed ('layout', 'ocr')\n    ) AS raw_text\nFROM pdf_files;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fe17b-9580-43b4-8b23-fb61a695df6c",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "ViewParsedText"
   },
   "outputs": [],
   "source": "-- check the RAW_TEXT to ensure the PDF was parsed as expected\n-- Complete the missing code (???) to check the RAW_TEXT to ensure the PDF was parsed as expected\n\nSELECT *, SNOWFLAKE.CORTEX.COUNT_TOKENS('mistral-7b', RAW_TEXT) as token_count\nFROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce5839-f0eb-4d4f-838e-9b3633979c8c",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "TestingSplitTextFunction",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Use Snowflake's new SPLIT_TEXT_RECURSIVE_CHARACTER feature to chunk parsed text from the PDFs loaded in @SKO_SKORAGHOP_LIVE_PROD.HOP.RAG stage\n-- Cortex SPLIT_TEXT_RECURSIVE_CHARACTER documentation link is https://docs.snowflake.com/sql-reference/functions/split_text_recursive_character-snowflake-cortex\n\n-- Complete the missing code (???) to:\n---- Create a new table called CORTEX_CHUNK to hold the chunked text from your PDF documents\n---- Use Cortex SPLIT_TEXT_RECURSIVE_CHARACTER feature with a 2000 chunk size and 100 overlap size\n\nCREATE OR REPLACE TABLE {{DB_NAME}}.{{SCHEMA_NAME}}.DOC_CHUNKS AS\nWITH text_chunks AS (\n    SELECT\n        relative_path,\n        SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n            raw_text:content::STRING,  -- Extract the 'content' field from the JSON\n            'markdown', -- Adjust to 'markdown' if needed\n            2000,       -- Adjust chunk size\n            100,        -- Adjust overlap size\n            ['\\n\\n', '\\n']    -- Adjust separators\n        ) AS chunks\n    FROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT\n)\nSELECT\n    relative_path,\n    c.value AS chunk  -- Extract each chunk of the parsed text\nFROM text_chunks,\nLATERAL FLATTEN(INPUT => chunks) c;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cfb61-0168-43ea-9a70-8242dc45ec07",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "ViewChunkedText",
    "resultHeight": 438
   },
   "outputs": [],
   "source": "-- check the CORTEX_CHUNK to ensure the PDF was chunked as expected\n-- Complete the missing code (???) to check the CORTEX_CHUNK to ensure the PDF was chunked as expected for the PDF called \"RAGWithoutAugmentation.pdf\"\n\nSELECT *, SNOWFLAKE.CORTEX.COUNT_TOKENS('mistral-7b', CHUNK) as token_count\nFROM {{DB_NAME}}.{{SCHEMA_NAME}}.DOC_CHUNKS \nWHERE RELATIVE_PATH = 'Cortex_TSI.pdf'"
  },
  {
   "cell_type": "code",
   "id": "51ffd4c9-fd4d-441b-bd9b-d755025a1acf",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "ALTER TABLE DOC_CHUNKS ADD COLUMN IF NOT EXISTS CHUNK_TOPIC VARCHAR(100);\n\nUPDATE DOC_CHUNKS \n    SET CHUNK_TOPIC = SNOWFLAKE.CORTEX.COMPLETE('llama4-maverick', \n    concat('Categorize the following text as one of the following categories \n    [Customer Reference, Code Example, Benchmark, Technical Blog] \n    and only return the name of the category. No additional text.', CHUNK));\n\nSELECT * FROM DOC_CHUNKS;    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7581dbe-14e8-4667-937c-dd404d6aaeef",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "SELECT * FROM DOC_CHUNKS",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1856f350-86ea-42e9-baf1-5da4165e73ef",
   "metadata": {
    "collapsed": false,
    "name": "Step2"
   },
   "source": [
    "## Step 2: Create Cortex Search Service\n",
    "Next, we create a [Cortex Search Service](https://docs.snowflake.com/LIMITEDACCESS/cortex-search/cortex-search-overview#overview) that enables retrieval of relevant text chunks for any query. This service uses the CHUNK column from the chunked table as the indexed content.\n",
    "\n",
    "Purpose: **Index and search chunked content to support the RAG pipeline.**\n",
    "\n",
    "Command:\n",
    "```sql\n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE SKO.HOP.RAG_SEARCH_SERVICE ON SEARCH_COL WAREHOUSE = COMPUTE_WH TARGET_LAG = '1 day' AS SELECT  ...;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e9ba5f5-d03b-44e8-a8d0-ed46770961f7",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "SELECT \n        ('DOCUMENT_TITLE: ' || RELATIVE_PATH || \n        '\\nDOCUMENT_TYPE: ' || CHUNK_TOPIC || \n        '\\nDOCUMENT_TEXT:\\n' || CHUNK) AS SEARCH_COL\n        FROM {{DB_NAME}}.{{SCHEMA_NAME}}.DOC_CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a330f9-c774-47f2-b8ca-031ae441c602",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CreateCortexSearchService",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Create a search service over your new chunked pdf table that has one searchable text\n\nCREATE OR REPLACE CORTEX SEARCH SERVICE {{DB_NAME}}.{{SCHEMA_NAME}}.SNOWFLAKE_BLOG_RETRIEVAL\n    ON SEARCH_COL\n    ATTRIBUTES CHUNK_TOPIC\n    WAREHOUSE = COMPUTE_WH\n    TARGET_LAG = '365 days'\n    AS SELECT \n        RELATIVE_PATH,\n        CHUNK_TOPIC,\n        ('DOCUMENT_TITLE: ' || RELATIVE_PATH || \n        '\\nDOCUMENT_TYPE: ' || CHUNK_TOPIC || \n        '\\nDOCUMENT_TEXT:\\n' || CHUNK) AS SEARCH_COL\n    FROM {{DB_NAME}}.{{SCHEMA_NAME}}.DOC_CHUNKS;"
  },
  {
   "cell_type": "markdown",
   "id": "348e3271-b238-465f-995c-3c8f1df0cf2b",
   "metadata": {
    "collapsed": false,
    "name": "Step3"
   },
   "source": [
    "## Step 3: Test Search Results with Experimental Configurations\n",
    "We will now evaluate [Snowflake Cortex Experimental Knobs](https://docs.google.com/document/d/1HkHtDiY3CmzpSewCe_s9fpMNE5spOUvNSwr6CxFerqE/edit?usp=sharing) to fine-tune the retrieval service and analyze confidence scores and result rankings across configurations. These tests focus on boosting, recency, headers, and reranking to optimize search relevance.\n",
    "\n",
    "**Configurations Tested:**\n",
    "- **Boosted vs. Unboosted:** Compare the impact of keyword emphasis on rankings and scores.\n",
    "- **Time-Based Decays:** Test how prioritizing recent documents affects relevance.\n",
    "- **Header Boosts:** Evaluate the influence of structured headers (e.g., Markdown) on ranking.\n",
    "- **Reranked vs. Non-Reranked:** Analyze trade-offs between query latency and search quality.\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Confidence Scores:** Global relevance scores (0â€“3) for each result.\n",
    "- **Result Rankings:** Position changes reveal the effectiveness of configurations.\n",
    "\n",
    "By testing these configurations, we aim to enhance Cortex Search Service performance for specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e406ce-4f72-4fd5-8360-fc85e10c73bc",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "CortexSearchEnhancements"
   },
   "outputs": [],
   "source": [
    "# Define image in a stage and read the file\n",
    "image=session.file.get_stream(\"@SKO_SKORAGHOP_LIVE_PROD.HOP.RAG/CortexSearchEnhancements.jpg\", decompress=False).read() \n",
    "st.image(image, width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8762e-0ebb-4aa7-8a2a-d4af9cbba828",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "TestCortexSearchExperimentalKnobs"
   },
   "outputs": [],
   "source": [
    "-- This query compares Cortex Search Service results across multiple experimental settings: \n",
    "---- boosted (using softBoosts), header boosted, and unboosted (default settings).\n",
    "\n",
    "-- The results are presented side by side to analyze the impact of each configuration on confidence scores and document ranking for matching search columns.\n",
    "-- This analysis helps evaluate the effectiveness of boosting and decay strategies in improving search relevance and recency-based ranking.\n",
    "\n",
    "-- Missing code (???) has been completed to:\n",
    "---- Call the SKO_SKORAGHOP_LIVE_PROD.HOP.RAG_SEARCH_SERVICE to test experimental configurations.\n",
    "---- Use the query: \"How can I augment my LLM prompts with relevant context in Snowpark?\"\n",
    "---- For the boosted_results section, apply softBoosts using the phrases \"Augment\" and \"RAG.\"\n",
    "---- Enable returnConfidenceScores to true for all configurations.\n",
    "\n",
    "WITH boosted_results AS (\n",
    "    SELECT DISTINCT\n",
    "        VALUE:\"SEARCH_COL\"::STRING AS SearchColumn,\n",
    "        VALUE:\"@CONFIDENCE_SCORE\"::STRING AS ConfidenceScore\n",
    "    FROM (\n",
    "        SELECT PARSE_JSON(\n",
    "            SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "                'SKO_SKORAGHOP_LIVE_PROD.HOP.RAG_SEARCH_SERVICE',\n",
    "                '{\n",
    "                    \"query\": \"How can I augment my llm prompts with relevant context in snowpark?\",\n",
    "                    \"limit\": 3,\n",
    "                    \"experimental\": {\n",
    "                        \"softBoosts\": [\n",
    "                            { \"phrase\": \"Augment\" },\n",
    "                            { \"phrase\": \"RAG\" }\n",
    "                        ],\n",
    "                        \"reranker\": \"none\",\n",
    "                        \"returnConfidenceScores\": true\n",
    "                    }\n",
    "                }'\n",
    "            )\n",
    "        ) AS boosted_json\n",
    "    ),\n",
    "    LATERAL FLATTEN(input => boosted_json:\"results\")\n",
    "),\n",
    "header_boosted_results AS (\n",
    "    SELECT DISTINCT\n",
    "        VALUE:\"SEARCH_COL\"::STRING AS SearchColumn,\n",
    "        VALUE:\"@CONFIDENCE_SCORE\"::STRING AS ConfidenceScore\n",
    "    FROM (\n",
    "        SELECT PARSE_JSON(\n",
    "            SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "                'SKO_SKORAGHOP_LIVE_PROD.HOP.RAG_SEARCH_SERVICE',\n",
    "                '{\n",
    "                    \"query\": \"How can I augment my llm prompts with relevant context in snowpark?\",\n",
    "                    \"limit\": 3,\n",
    "                    \"experimental\": {\n",
    "                        \"headerBoost\": {\n",
    "                            \"multiplier\": 2,\n",
    "                            \"skipStopWords\": true\n",
    "                        },\n",
    "                        \"reranker\": \"none\",\n",
    "                        \"returnConfidenceScores\": true\n",
    "                    }\n",
    "                }'\n",
    "            )\n",
    "        ) AS header_boosted_json\n",
    "    ),\n",
    "    LATERAL FLATTEN(input => header_boosted_json:\"results\")\n",
    "),\n",
    "unboosted_results AS (\n",
    "    SELECT DISTINCT\n",
    "        VALUE:\"SEARCH_COL\"::STRING AS SearchColumn,\n",
    "        VALUE:\"@CONFIDENCE_SCORE\"::STRING AS ConfidenceScore\n",
    "    FROM (\n",
    "        SELECT PARSE_JSON(\n",
    "            SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "                'SKO_SKORAGHOP_LIVE_PROD.HOP.RAG_SEARCH_SERVICE',\n",
    "                '{\n",
    "                    \"query\": \"How can I augment my llm prompts with relevant context in snowpark?\",\n",
    "                    \"limit\": 3,\n",
    "                    \"experimental\": {\n",
    "                        \"returnConfidenceScores\": true\n",
    "                    }\n",
    "                }'\n",
    "            )\n",
    "        ) AS unboosted_json\n",
    "    ),\n",
    "    LATERAL FLATTEN(input => unboosted_json:\"results\")\n",
    ")\n",
    "SELECT \n",
    "    COALESCE(b.SearchColumn, hb.SearchColumn, u.SearchColumn) AS SearchColumn,\n",
    "    b.ConfidenceScore AS BoostedConfidenceScore,\n",
    "    hb.ConfidenceScore AS HeaderBoostedConfidenceScore,\n",
    "    u.ConfidenceScore AS UnboostedConfidenceScore\n",
    "FROM\n",
    "    boosted_results b\n",
    "FULL OUTER JOIN header_boosted_results hb\n",
    "    ON b.SearchColumn = hb.SearchColumn\n",
    "FULL OUTER JOIN unboosted_results u\n",
    "    ON COALESCE(b.SearchColumn, hb.SearchColumn) = u.SearchColumn\n",
    "ORDER BY \n",
    "    CASE WHEN BoostedConfidenceScore IS NULL THEN 1 ELSE 0 END, \n",
    "    BoostedConfidenceScore DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859c78c-4bd3-4bf6-b355-68f357830b74",
   "metadata": {
    "collapsed": false,
    "name": "Step4"
   },
   "source": [
    "## Step 4: Pass Retrieved Content to LLMs\n",
    "This step demonstrates how to pass retrieved contextual content to various LLMs using the Snowflake Cortex [`COMPLETE`](https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex) function. The process includes:\n",
    "\n",
    "- **Retrieving Contextual Information**: Context is fetched from the search service.\n",
    "- **Generating Structured Prompts**: The retrieved context is injected into prompts for LLMs.\n",
    "- **LLM Interaction**: Prompts are passed to models like `mistral-7b`, `mistral-large2`, and `Anthropic Claude 3.5` for response generation.\n",
    "- **Comparative Analysis**: Model outputs are compared for quality, relevance, and coherence.\n",
    "\n",
    "Example Query:\n",
    "```sql\n",
    "SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "    'claude-3-5-sonnet',\n",
    "    CONCAT('Your context: ', (SELECT LISTAGG(CHUNK, ' ') FROM searchresults))\n",
    ") AS RESPONSE\n",
    "FROM searchresults;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f5809-2ca9-4908-9d1a-1ad6fba7db29",
   "metadata": {
    "collapsed": false,
    "name": "QueryIdeas",
    "resultHeight": 169
   },
   "source": [
    "**Queries to test the capabilities of the LLMs based on the PDF content:**\n",
    "- What is the difference between semantic and lexical searches? Does a hybrid system exist?\n",
    "- How can we optimize context retrieval in retrievel agumented geneartion for an LLM system?\"'\n",
    "- Can I use SQL in Snowflake to retrieve relevant context for my GPT prompt?\n",
    "- What service runs fuzzy-search to retrieve context in Snowflake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19284d0-107c-4c7f-aa2e-5378159c4a3b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "CheckSearchResults"
   },
   "outputs": [],
   "source": "# Query your Snowflake Cortex Search Service using the Snowpark Python API to retrieve and process search results.\n\n# Complete the missing code (???) to:\n## Specify your database 'SKO_SKORAGHOP_LIVE_PROD', your schema 'HOP', and your Cortex Search Service named 'RAG_SEARCH_SERVICE'\n## Specify your SEARCH_COL as the column of interest\n\nfrom snowflake.snowpark import Session\nfrom snowflake.core import Root\nroot = Root(session)\n\ntranscript_search_service = (root\n  .databases[DB_NAME]\n  .schemas[SCHEMA_NAME]\n  .cortex_search_services['SNOWFLAKE_BLOG_RETRIEVEL']\n)\n\nresp = transcript_search_service.search(\n  query=\"\"\"How does Snowflake simplify the deployment of retrieval-augmented generation (RAG) workflows?\"\"\",\n  columns=['SEARCH_COL'],\n  limit=3\n)\nresults = resp.results\n\ncontext_str = \"\"\nfor i, r in enumerate(results):\n    context_str += f\"Context document {i+1}: {r['SEARCH_COL']}\\n****************\\n\"\n\nprint(context_str)\ndf = session.create_dataframe(resp.results)\ndf.create_or_replace_temp_view(\"searchresults\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65daf34a-f360-4223-bff2-60f016ff6c3e",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "CortexSearchRetrieverCell"
   },
   "outputs": [],
   "source": [
    "# Define the retriever class for interacting with the Cortex Search Service\n",
    "\n",
    "# Complete the missing code (???) to:\n",
    "## Specify your database 'SKO_SKORAGHOP_LIVE_PROD', your schema 'HOP', and your Cortex Search Service named 'RAG_SEARCH_SERVICE'\n",
    "## Specify your SEARCH_COL as the column of interest\n",
    "## Intialize retriever with your CortexSearchRetriever class\n",
    "## Use \"What are some components of the Snowflake Cortex offering? How do they work?\" for the test_query\n",
    "\n",
    "from typing import List\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.core import Root\n",
    "\n",
    "# CortexSearchRetriever\n",
    "class CortexSearchRetriever:\n",
    "    def __init__(self, session: Session, limit_to_retrieve: int = 4):\n",
    "        self._session = session\n",
    "        self._limit_to_retrieve = limit_to_retrieve\n",
    "        \n",
    "\n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        root = Root(session)\n",
    "        cortex_search_service = (\n",
    "            root\n",
    "            .databases[\"SKO_SKORAGHOP_LIVE_PROD\"]\n",
    "            .schemas[\"HOP\"]\n",
    "            .cortex_search_services[\"RAG_SEARCH_SERVICE\"]\n",
    "        )\n",
    "        resp = cortex_search_service.search(\n",
    "            query=query,\n",
    "            columns=[\"SEARCH_COL\"],\n",
    "            limit=self._limit_to_retrieve,\n",
    "        )\n",
    "        return [row[\"SEARCH_COL\"] for row in resp.results] if resp.results else []\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = CortexSearchRetriever(session=session, limit_to_retrieve=3)\n",
    "test_query = \"What are some components of the Snowflake Cortex offering? How do they work?\"\n",
    "retrieved_context = retriever.retrieve(query=test_query)\n",
    "print(retrieved_context)"
   ]
  }
 ]
}